{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4f4718-3f90-46b5-bd6c-3105cecefaaf",
   "metadata": {},
   "source": [
    "# The customer asked for two more things.\n",
    "1) The images are too small they need to be 500x500px\n",
    "2) Need the catalog the be in word format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c5bc5-4057-4d36-b93f-cae80da5a1e3",
   "metadata": {},
   "source": [
    "The second of these is relatively easy - just ask ChatGPT to produce the output in .docx format. There is a library for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8038f6-884e-4740-84c8-3c08c1f7c37a",
   "metadata": {},
   "source": [
    "The first requires some re-thinking. Currently we are scraping Google's image search page. This page does not contain the images but rather thumbnails of the images. The first thought is to access the real image by following the link beck to the origional image. Modern web pages are heavily saturated with Java script making this tricky. Another way to approach this is to use asearch that does find the real images. Google, Microsoft and others provide such a tool but you need to sign up for their API and pay a fee.\n",
    "I chose to go with SERP API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6829ac9-bbdb-43be-86e7-cf3998f8106d",
   "metadata": {},
   "source": [
    "A SERP API, or Search Engine Results Page API, is a tool that allows developers to programmatically access and retrieve data from search engine results pages (SERPs). Essentially, it's an automated way to scrape and parse the information displayed on a search engine results page, like Google or Bing, for a specific query. This data can include things like organic search results, ads, knowledge graphs, and other SERP features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895497f1-4d5c-44a8-b0d6-83a2596c1c3c",
   "metadata": {},
   "source": [
    "Sign up for a free Serp AIP key at https://serpapi.com/\n",
    "Note: The Serp API has a limited free offering so make sure not to make unnecessary calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c1e20-803b-4aaa-ae75-1c3ca720b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-docx requests beautifulsoup4 fpdf pillow pandas reportlab google-api-python-client selenium google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b544c01-1cd4-40ea-a590-5c7daab9dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from serpapi import GoogleSearch\n",
    "from requests.exceptions import SSLError\n",
    "\n",
    "# Constants\n",
    "DATA_FILE = \"data/materials.csv\"\n",
    "IMAGE_DIR = \"data/images/part5\"\n",
    "BRAND_IMAGE_DIR = os.path.join(IMAGE_DIR, \"brands\")\n",
    "FABRIC_IMAGE_DIR = os.path.join(IMAGE_DIR, \"fabrics\")\n",
    "OUTPUT_DOCX = \"data/material_catalog_part5.docx\"\n",
    "SERPAPI_KEY = \"359e12bf53e67836e1989b8edcd1e78d12020ff3d100ac33865e63eee371191b\"  # Replace with your actual SerpAPI key\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(BRAND_IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(FABRIC_IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "def is_valid_image_url(url):\n",
    "    domain = urlparse(url).netloc.lower()\n",
    "    if \"instagram.com\" in domain:\n",
    "        print(f\"‚õî Skipping Instagram URL: {url}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def fetch_image_url_google_scrape(query):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    search_url = f\"https://www.google.com/search?tbm=isch&q={urllib.parse.quote(query)}\"\n",
    "    try:\n",
    "        response = requests.get(search_url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        images = soup.find_all(\"img\")\n",
    "\n",
    "        for img in images:\n",
    "            src = img.get(\"src\")\n",
    "            if src and src.startswith(\"http\") and is_valid_image_url(src):\n",
    "                return src\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to fetch image for {query}: {e}\")\n",
    "    return None\n",
    "\n",
    "def fetch_image_url_serpapi(query):\n",
    "    try:\n",
    "        search = GoogleSearch({\n",
    "            \"q\": query,\n",
    "            \"tbm\": \"isch\",\n",
    "            \"api_key\": SERPAPI_KEY\n",
    "        })\n",
    "        results = search.get_dict()\n",
    "        for img in results.get(\"images_results\", []):\n",
    "            url = img.get(\"original\")\n",
    "            if is_valid_image_url(url):\n",
    "                try:\n",
    "                    head = requests.head(url, timeout=5, allow_redirects=True)\n",
    "                    if head.status_code == 200 and \"image\" in head.headers.get(\"Content-Type\", \"\"):\n",
    "                        return url\n",
    "                except SSLError as ssl_err:\n",
    "                    print(f\"‚õî SSL error when checking: {url} ‚Äî {ssl_err}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error checking image URL {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå SerpAPI failed for {query}: {e}\")\n",
    "    return None\n",
    "\n",
    "def download_image(url, save_path):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "        if \"image\" not in content_type:\n",
    "            print(f\"‚ö†Ô∏è Not an image (Content-Type={content_type}): {url}\")\n",
    "            with open(save_path + \".html\", \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            return None\n",
    "\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        image.save(save_path)\n",
    "        return save_path\n",
    "    except SSLError as ssl_err:\n",
    "        print(f\"‚õî SSL error for {url}: {ssl_err}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to download image from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_docx_from_data(df):\n",
    "    doc = Document()\n",
    "    doc.add_heading(\"Fabric Catalog\", 0)\n",
    "\n",
    "    table = doc.add_table(rows=0, cols=2)\n",
    "    table.autofit = True\n",
    "    table.style = \"Table Grid\"\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        brand = row[\"Brand\"]\n",
    "        product_name = row[\"Product Name\"]\n",
    "        product_code = row[\"Product Code\"]\n",
    "        color = row[\"Color\"]\n",
    "        notes = row[\"Notes\"]\n",
    "\n",
    "        fabric_img_path = os.path.join(FABRIC_IMAGE_DIR, f\"{brand}_{product_name}.jpg\".replace(\" \", \"_\"))\n",
    "        brand_img_path = os.path.join(BRAND_IMAGE_DIR, f\"{brand}.jpg\".replace(\" \", \"_\"))\n",
    "\n",
    "        row_cells = table.add_row().cells\n",
    "\n",
    "        if os.path.exists(fabric_img_path):\n",
    "            paragraph = row_cells[0].paragraphs[0]\n",
    "            run = paragraph.add_run()\n",
    "            run.add_picture(fabric_img_path, width=Inches(1.5))\n",
    "        else:\n",
    "            row_cells[0].text = \"No Image\"\n",
    "\n",
    "        if os.path.exists(brand_img_path):\n",
    "            run = row_cells[1].paragraphs[0].add_run()\n",
    "            run.add_picture(brand_img_path, width=Inches(1.0))\n",
    "\n",
    "        row_cells[1].add_paragraph(f\"Brand: {brand}\")\n",
    "        row_cells[1].add_paragraph(f\"Product: {product_name}\")\n",
    "        row_cells[1].add_paragraph(f\"Code: {product_code}\")\n",
    "        row_cells[1].add_paragraph(f\"Color: {color}\")\n",
    "        row_cells[1].add_paragraph(f\"Notes: {notes}\")\n",
    "\n",
    "        if (idx + 1) % 8 == 0:\n",
    "            doc.add_paragraph(\"\\n\")\n",
    "\n",
    "    doc.save(OUTPUT_DOCX)\n",
    "    print(f\"‚úÖ DOCX saved to: {OUTPUT_DOCX}\")\n",
    "\n",
    "# Main Script\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    brand = row[\"Brand\"]\n",
    "    product_name = row[\"Product Name\"]\n",
    "    product_code = row[\"Product Code\"]\n",
    "    color = row[\"Color\"]\n",
    "\n",
    "    brand_img_path = os.path.join(BRAND_IMAGE_DIR, f\"{brand}.jpg\".replace(\" \", \"_\"))\n",
    "    if not os.path.exists(brand_img_path):\n",
    "        brand_url = fetch_image_url_google_scrape(f\"{brand} logo\")\n",
    "        if brand_url:\n",
    "            download_image(brand_url, brand_img_path)\n",
    "\n",
    "    fabric_img_path = os.path.join(FABRIC_IMAGE_DIR, f\"{brand}_{product_name}.jpg\".replace(\" \", \"_\"))\n",
    "    if not os.path.exists(fabric_img_path):\n",
    "        query_parts = [brand, product_name]\n",
    "        if product_code.lower() != \"unknown\":\n",
    "            query_parts.append(product_code)\n",
    "        if color.lower() != \"unknown\":\n",
    "            query_parts.append(color)\n",
    "        fabric_query = \" \".join(query_parts)\n",
    "\n",
    "        fabric_url = fetch_image_url_serpapi(fabric_query)\n",
    "        if fabric_url:\n",
    "            print(f\"üì• Downloading image for: {fabric_query}\")\n",
    "            print(f\"üîó Image URL: {fabric_url}\")\n",
    "            download_image(fabric_url, fabric_img_path)\n",
    "\n",
    "create_docx_from_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9943f137-f24f-4805-94de-46aaebf21e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter)",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
